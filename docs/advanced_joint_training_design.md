# ğŸš€ å…ˆè¿›è”åˆè®­ç»ƒè®¾è®¡æ–¹æ¡ˆ

## ğŸ’¡ æ ¸å¿ƒé—®é¢˜åˆ†æ

### å½“å‰æ¨¡å‹çš„å±€é™æ€§

1. **ç®€å•æŸå¤±åŠ æƒ**: `loss = 0.4 * link_loss + 0.6 * rumor_loss`
2. **å•å‘ä¿¡æ¯æµ**: CascadePredictor â†’ RumorDetectorï¼Œç¼ºä¹åå‘æŒ‡å¯¼
3. **é™æ€æƒé‡**: å›ºå®šçš„ä»»åŠ¡æƒé‡ï¼Œæ— æ³•é€‚åº”ä¸åŒè®­ç»ƒé˜¶æ®µ
4. **æµ…å±‚äº¤äº’**: åªåœ¨æœ€ç»ˆè¾“å‡ºå±‚è¿›è¡Œç®€å•çš„å›¾é‡æ„

## ğŸ¯ è®¾è®¡ç›®æ ‡

1. **æ·±åº¦ä»»åŠ¡äº¤äº’**: è®©ä¸¤ä¸ªä»»åŠ¡åœ¨å¤šä¸ªå±‚æ¬¡ä¸Šç›¸äº’æŒ‡å¯¼å’Œå¢å¼º
2. **è‡ªé€‚åº”æƒé‡**: æ ¹æ®è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒæ•´ä»»åŠ¡é‡è¦æ€§
3. **ä¿¡æ¯ä¼ é€’æœºåˆ¶**: è®¾è®¡æœ‰æ•ˆçš„è·¨ä»»åŠ¡ä¿¡æ¯ä¼ é€’é€šé“
4. **è¡¨ç¤ºå¯¹é½**: ç¡®ä¿ä¸¤ä¸ªä»»åŠ¡å­¦ä¹ åˆ°ä¸€è‡´ä¸”äº’è¡¥çš„è¡¨ç¤º

---

## ğŸ—ï¸ æ–¹æ¡ˆ1: åŸºäºæ³¨æ„åŠ›çš„åŒå‘ä¿¡æ¯

### æ ¸å¿ƒæ€æƒ³
è®©CascadePredictorå’ŒRumorDetectoré€šè¿‡å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæ·±åº¦äº¤äº’ï¼Œå®ç°åŒå‘ä¿¡æ¯ä¼ é€’ã€‚

CascadePredictor â†’ RumorDetector: ä¼ æ’­æ¨¡å¼ç‰¹å¾
RumorDetector â†’ CascadePredictor: è¯­ä¹‰ç†è§£ç‰¹å¾

### æ¶æ„è®¾è®¡

```python
class BidirectionalAttentionFramework(nn.Module):
    """
    åŒå‘æ³¨æ„åŠ›è”åˆè®­ç»ƒæ¡†æ¶
    """
    def __init__(self, shared_encoder, hidden_dim=512, num_heads=8):
        super().__init__()
        self.shared_encoder = shared_encoder
        self.hidden_dim = hidden_dim
        
        # ä»»åŠ¡ç‰¹å®šç¼–ç å™¨
        self.cascade_encoder = CascadeSpecificEncoder(hidden_dim)
        self.rumor_encoder = RumorSpecificEncoder(hidden_dim)
        
        # åŒå‘æ³¨æ„åŠ›æœºåˆ¶
        self.cascade_to_rumor_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=num_heads, batch_first=True
        )
        self.rumor_to_cascade_attn = nn.MultiheadAttention(
            embed_dim=hidden_dim, num_heads=num_heads, batch_first=True
        )
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.feature_fusion = FeatureFusionNetwork(hidden_dim)
        
        # æœ€ç»ˆé¢„æµ‹å¤´
        self.cascade_predictor = EnhancedCascadePredictor(hidden_dim)
        self.rumor_classifier = EnhancedRumorClassifier(hidden_dim)

    def forward(self, data):
        # 1. å…±äº«ç‰¹å¾æå–
        shared_features = self.shared_encoder(data.x, data.edge_index)
        
        # 2. ä»»åŠ¡ç‰¹å®šç‰¹å¾ç¼–ç 
        cascade_features = self.cascade_encoder(shared_features, data.edge_index)
        rumor_features = self.rumor_encoder(shared_features, data.edge_index)
        
        # 3. åŒå‘æ³¨æ„åŠ›äº¤äº’
        # CascadePredictor â†’ RumorDetector: ä¼ æ’­æ¨¡å¼æŒ‡å¯¼è¯­ä¹‰ç†è§£
        enhanced_rumor_feat, cascade_to_rumor_weights = self.cascade_to_rumor_attn(
            query=rumor_features,
            key=cascade_features, 
            value=cascade_features
        )
        
        # RumorDetector â†’ CascadePredictor: è¯­ä¹‰ä¿¡æ¯æŒ‡å¯¼ä¼ æ’­é¢„æµ‹
        enhanced_cascade_feat, rumor_to_cascade_weights = self.rumor_to_cascade_attn(
            query=cascade_features,
            key=rumor_features,
            value=rumor_features
        )
        
        # 4. ç‰¹å¾èåˆ
        fused_cascade = self.feature_fusion(cascade_features, enhanced_cascade_feat)
        fused_rumor = self.feature_fusion(rumor_features, enhanced_rumor_feat)
        
        # 5. æœ€ç»ˆé¢„æµ‹
        link_predictions = self.cascade_predictor(fused_cascade, data.pred_edge_index)
        rumor_predictions = self.rumor_classifier(fused_rumor, data.batch)
        
        return {
            'rumor_logits': rumor_predictions,
            'link_probs': link_predictions,
            'cascade_features': fused_cascade,
            'rumor_features': fused_rumor,
            'attention_weights': {
                'cascade_to_rumor': cascade_to_rumor_weights,
                'rumor_to_cascade': rumor_to_cascade_weights
            }
        }
```

### ä¿¡æ¯ä¼ é€’æœºåˆ¶

1. **ä¼ æ’­æ¨¡å¼ â†’ è¯­ä¹‰ç†è§£**
   - CascadePredictorå­¦åˆ°çš„ä¼ æ’­æ¨¡å¼ç‰¹å¾æŒ‡å¯¼RumorDetectorç†è§£å†…å®¹è¯­ä¹‰
   - ä¾‹å¦‚ï¼šç—…æ¯’å¼ä¼ æ’­æ¨¡å¼å¯èƒ½æš—ç¤ºå†…å®¹çš„ç…½åŠ¨æ€§

2. **è¯­ä¹‰ç†è§£ â†’ ä¼ æ’­é¢„æµ‹**
   - RumorDetectorçš„è¯­ä¹‰ç†è§£æŒ‡å¯¼CascadePredictoré¢„æµ‹åˆç†çš„ä¼ æ’­è·¯å¾„
   - ä¾‹å¦‚ï¼šè´Ÿé¢æƒ…æ„Ÿå†…å®¹å¯èƒ½å¯¼è‡´æ›´æ¿€çƒˆçš„ä¼ æ’­

---

## ğŸ—ï¸ æ–¹æ¡ˆ2: å…ƒå­¦ä¹ è‡ªé€‚åº”æƒé‡è°ƒæ•´

### æ ¸å¿ƒæ€æƒ³
é€šè¿‡å…ƒå­¦ä¹ åŠ¨æ€å­¦ä¹ ä¸¤ä¸ªä»»åŠ¡çš„æœ€ä¼˜æƒé‡ç»„åˆï¼Œè€Œä¸æ˜¯ä½¿ç”¨å›ºå®šçš„æƒé‡ã€‚

### è®¾è®¡æ¶æ„

```python
class MetaAdaptiveWeighting(nn.Module):
    """
    åŸºäºå…ƒå­¦ä¹ çš„è‡ªé€‚åº”æƒé‡è°ƒæ•´
    """
    def __init__(self, feature_dim, meta_lr=0.001):
        super().__init__()
        self.meta_lr = meta_lr
        
        # æƒé‡é¢„æµ‹ç½‘ç»œ
        self.weight_predictor = nn.Sequential(
            nn.Linear(feature_dim * 3, 128),  # cascade + rumor + global features
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 2),  # [w_cascade, w_rumor]
            nn.Softmax(dim=-1)
        )
        
        # å…ƒå­¦ä¹ ä¼˜åŒ–å™¨
        self.meta_optimizer = torch.optim.Adam(self.weight_predictor.parameters(), lr=meta_lr)
        
    def compute_adaptive_weights(self, cascade_features, rumor_features, global_context):
        """
        åŸºäºå½“å‰ç‰¹å¾çŠ¶æ€è®¡ç®—è‡ªé€‚åº”æƒé‡
        """
        # èšåˆç‰¹å¾
        cascade_global = torch.mean(cascade_features, dim=0)
        rumor_global = torch.mean(rumor_features, dim=0)
        
        # ç»„åˆç‰¹å¾
        combined_features = torch.cat([cascade_global, rumor_global, global_context], dim=-1)
        
        # é¢„æµ‹æƒé‡
        weights = self.weight_predictor(combined_features)
        return weights[0], weights[1]  # w_cascade, w_rumor
    
    def meta_update(self, val_loss):
        """
        åŸºäºéªŒè¯æŸå¤±è¿›è¡Œå…ƒå­¦ä¹ æ›´æ–°
        """
        self.meta_optimizer.zero_grad()
        val_loss.backward(retain_graph=True)
        self.meta_optimizer.step()

class AdaptiveJointLoss(nn.Module):
    def __init__(self, feature_dim, temperature=1.0):
        super().__init__()
        self.meta_weighter = MetaAdaptiveWeighting(feature_dim)
        self.temperature = temperature
        
    def forward(self, cascade_loss, rumor_loss, cascade_features, rumor_features, epoch):
        # å…¨å±€ä¸Šä¸‹æ–‡ï¼šè®­ç»ƒè¿›åº¦ã€æŸå¤±å†å²ç­‰
        global_context = self.compute_global_context(epoch, cascade_loss, rumor_loss)
        
        # è®¡ç®—è‡ªé€‚åº”æƒé‡
        w_cascade, w_rumor = self.meta_weighter.compute_adaptive_weights(
            cascade_features, rumor_features, global_context
        )
        
        # è‡ªé€‚åº”æŸå¤±
        adaptive_loss = w_cascade * cascade_loss + w_rumor * rumor_loss
        
        # æƒé‡å¹³è¡¡æ­£åˆ™åŒ–
        weight_entropy = -w_cascade * torch.log(w_cascade + 1e-8) - w_rumor * torch.log(w_rumor + 1e-8)
        balance_penalty = -0.1 * weight_entropy  # é¼“åŠ±æƒé‡å¤šæ ·æ€§
        
        total_loss = adaptive_loss + balance_penalty
        
        return total_loss, w_cascade, w_rumor
```

---

## ğŸ—ï¸ æ–¹æ¡ˆ3: å¯¹æŠ—æ€§è”åˆè®­ç»ƒæ¡†æ¶

### æ ¸å¿ƒæ€æƒ³
å¼•å…¥åˆ¤åˆ«å™¨ï¼Œé€šè¿‡å¯¹æŠ—æ€§è®­ç»ƒæå‡CascadePredictorç”Ÿæˆä¼ æ’­å›¾çš„è´¨é‡ã€‚

### æ¶æ„è®¾è®¡

```python
class AdversarialJointFramework(nn.Module):
    """
    å¯¹æŠ—æ€§è”åˆè®­ç»ƒæ¡†æ¶
    """
    def __init__(self, config):
        super().__init__()
        # ä¸»è¦ç»„ä»¶
        self.generator = JointModel(config)  # CascadePredictor + RumorDetector
        self.discriminator = PropagationDiscriminator(config.hidden_dim)
        
        # å¯¹æŠ—æ€§æŸå¤±æƒé‡
        self.adv_weight = config.adversarial_weight
        
    def forward(self, batch, mode='train'):
        if mode == 'train':
            return self.adversarial_training_step(batch)
        else:
            return self.generator(batch)
    
    def adversarial_training_step(self, batch):
        # 1. Generatorå‰å‘ä¼ æ’­
        gen_outputs = self.generator(batch)
        
        # 2. æ„é€ çœŸå®vsé¢„æµ‹ä¼ æ’­å›¾
        real_propagation = self.construct_real_propagation(batch)
        fake_propagation = self.construct_predicted_propagation(batch, gen_outputs['link_probs'])
        
        # 3. åˆ¤åˆ«å™¨è¯„åˆ†
        real_scores = self.discriminator(real_propagation)
        fake_scores = self.discriminator(fake_propagation)
        
        # 4. å¯¹æŠ—æ€§æŸå¤±
        generator_adv_loss = self.compute_generator_adversarial_loss(fake_scores)
        discriminator_loss = self.compute_discriminator_loss(real_scores, fake_scores)
        
        # 5. æ€»æŸå¤±
        main_loss = self.compute_main_task_loss(gen_outputs, batch)
        total_generator_loss = main_loss + self.adv_weight * generator_adv_loss
        
        return {
            'generator_loss': total_generator_loss,
            'discriminator_loss': discriminator_loss,
            'outputs': gen_outputs,
            'adversarial_metrics': {
                'real_scores': real_scores,
                'fake_scores': fake_scores
            }
        }

class PropagationDiscriminator(nn.Module):
    """
    ä¼ æ’­å›¾åˆ¤åˆ«å™¨ï¼šåŒºåˆ†çœŸå®ä¼ æ’­vsé¢„æµ‹ä¼ æ’­
    """
    def __init__(self, hidden_dim):
        super().__init__()
        self.graph_encoder = GraphEncoder(hidden_dim)
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, propagation_graph):
        graph_repr = self.graph_encoder(propagation_graph)
        authenticity_score = self.classifier(graph_repr)
        return authenticity_score
```

---

## ğŸ—ï¸ æ–¹æ¡ˆ4: å±‚æ¬¡åŒ–ç‰¹å¾å¯¹é½

### æ ¸å¿ƒæ€æƒ³
åœ¨å¤šä¸ªæŠ½è±¡å±‚æ¬¡ä¸Šå¯¹é½ä¸¤ä¸ªä»»åŠ¡çš„ç‰¹å¾è¡¨ç¤ºï¼Œç¡®ä¿å­¦ä¹ åˆ°ä¸€è‡´ä¸”äº’è¡¥çš„è¡¨ç¤ºã€‚

### è®¾è®¡æ¶æ„

```python
class HierarchicalFeatureAlignment(nn.Module):
    """
    å±‚æ¬¡åŒ–ç‰¹å¾å¯¹é½æ¡†æ¶
    """
    def __init__(self, hidden_dims=[128, 256, 512]):
        super().__init__()
        self.num_levels = len(hidden_dims)
        self.hidden_dims = hidden_dims
        
        # å¤šå±‚ç‰¹å¾æå–å™¨
        self.cascade_extractors = nn.ModuleList([
            nn.Linear(hidden_dims[i], hidden_dims[i]) for i in range(self.num_levels)
        ])
        self.rumor_extractors = nn.ModuleList([
            nn.Linear(hidden_dims[i], hidden_dims[i]) for i in range(self.num_levels)
        ])
        
        # å±‚æ¬¡åŒ–å¯¹é½æ¨¡å—
        self.aligners = nn.ModuleList([
            FeatureAligner(hidden_dims[i]) for i in range(self.num_levels)
        ])
        
        # è·¨å±‚æ³¨æ„åŠ›
        self.cross_level_attention = CrossLevelAttention(hidden_dims)
        
    def forward(self, shared_features):
        cascade_pyramid = []
        rumor_pyramid = []
        alignment_losses = []
        
        # æ„å»ºç‰¹å¾é‡‘å­—å¡”
        for level in range(self.num_levels):
            # æå–å±‚æ¬¡åŒ–ç‰¹å¾
            cascade_feat = self.cascade_extractors[level](shared_features)
            rumor_feat = self.rumor_extractors[level](shared_features)
            
            # ç‰¹å¾å¯¹é½
            aligned_cascade, aligned_rumor, align_loss = self.aligners[level](
                cascade_feat, rumor_feat
            )
            
            cascade_pyramid.append(aligned_cascade)
            rumor_pyramid.append(aligned_rumor)
            alignment_losses.append(align_loss)
        
        # è·¨å±‚ç‰¹å¾èåˆ
        fused_cascade = self.cross_level_attention(cascade_pyramid)
        fused_rumor = self.cross_level_attention(rumor_pyramid)
        
        return {
            'cascade_features': fused_cascade,
            'rumor_features': fused_rumor,
            'alignment_loss': sum(alignment_losses),
            'feature_pyramids': {
                'cascade': cascade_pyramid,
                'rumor': rumor_pyramid
            }
        }

class FeatureAligner(nn.Module):
    """
    ç‰¹å¾å¯¹é½æ¨¡å—
    """
    def __init__(self, feature_dim):
        super().__init__()
        self.projection = nn.Linear(feature_dim, feature_dim)
        self.similarity_metric = nn.CosineSimilarity(dim=-1)
        
    def forward(self, cascade_feat, rumor_feat):
        # ç‰¹å¾æŠ•å½±
        projected_cascade = self.projection(cascade_feat)
        projected_rumor = self.projection(rumor_feat)
        
        # å¯¹é½æŸå¤±ï¼šæœ€å¤§åŒ–ç›¸ä¼¼æ€§
        similarity = self.similarity_metric(projected_cascade, projected_rumor)
        alignment_loss = 1.0 - similarity.mean()
        
        return projected_cascade, projected_rumor, alignment_loss
```

---

## ğŸ—ï¸ æ–¹æ¡ˆ5: åŸºäºå›¾å¯¹æ¯”å­¦ä¹ çš„è”åˆä¼˜åŒ–

### æ ¸å¿ƒæ€æƒ³
é€šè¿‡å¯¹æ¯”å­¦ä¹ è®©ä¸¤ä¸ªä»»åŠ¡å­¦ä¹ åˆ°äº’è¡¥çš„è¡¨ç¤ºï¼Œæ­£æ ·æœ¬æ˜¯åŒä¸€æ ·æœ¬çš„ä¸åŒä»»åŠ¡è¡¨ç¤ºï¼Œè´Ÿæ ·æœ¬æ˜¯ä¸åŒæ ·æœ¬çš„è¡¨ç¤ºã€‚

### è®¾è®¡æ¶æ„

```python
class ContrastiveJointLearning(nn.Module):
    """
    åŸºäºå›¾å¯¹æ¯”å­¦ä¹ çš„è”åˆä¼˜åŒ–
    """
    def __init__(self, temperature=0.07, negative_samples=32):
        super().__init__()
        self.temperature = temperature
        self.negative_samples = negative_samples
        
        # æŠ•å½±å¤´
        self.cascade_projector = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 128)
        )
        self.rumor_projector = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(), 
            nn.Linear(256, 128)
        )
        
    def forward(self, cascade_features, rumor_features, labels):
        # ç‰¹å¾æŠ•å½±
        cascade_proj = F.normalize(self.cascade_projector(cascade_features), dim=-1)
        rumor_proj = F.normalize(self.rumor_projector(rumor_features), dim=-1)
        
        # å¯¹æ¯”å­¦ä¹ æŸå¤±
        contrastive_loss = self.compute_contrastive_loss(cascade_proj, rumor_proj, labels)
        
        # ä»»åŠ¡é—´ä¸€è‡´æ€§æŸå¤±
        consistency_loss = self.compute_consistency_loss(cascade_proj, rumor_proj)
        
        return contrastive_loss + 0.1 * consistency_loss
    
    def compute_contrastive_loss(self, cascade_repr, rumor_repr, labels):
        batch_size = cascade_repr.size(0)
        
        # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
        sim_matrix = torch.mm(cascade_repr, rumor_repr.t()) / self.temperature
        
        # æ­£æ ·æœ¬ï¼šå¯¹è§’çº¿å…ƒç´ ï¼ˆåŒä¸€æ ·æœ¬çš„ä¸åŒä»»åŠ¡è¡¨ç¤ºï¼‰
        pos_sim = torch.diag(sim_matrix)
        
        # InfoNCEæŸå¤±
        exp_sim = torch.exp(sim_matrix)
        neg_sum = torch.sum(exp_sim, dim=1) - torch.exp(pos_sim)
        
        loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + neg_sum))
        
        return loss.mean()
    
    def compute_consistency_loss(self, cascade_repr, rumor_repr):
        """
        è®¡ç®—ä»»åŠ¡é—´è¡¨ç¤ºä¸€è‡´æ€§æŸå¤±
        """
        # åŒä¸€æ ·æœ¬çš„ä¸¤ä¸ªä»»åŠ¡è¡¨ç¤ºåº”è¯¥ç›¸ä¼¼
        consistency = F.cosine_similarity(cascade_repr, rumor_repr, dim=-1)
        return 1.0 - consistency.mean()
```

---

## ğŸš€ å®æ–½å»ºè®®

### æ¸è¿›å¼å®æ–½è·¯å¾„

1. **ç¬¬ä¸€æ­¥**: å®ç°åŒå‘æ³¨æ„åŠ›æœºåˆ¶ (æ–¹æ¡ˆ1)
2. **ç¬¬äºŒæ­¥**: æ·»åŠ è‡ªé€‚åº”æƒé‡å­¦ä¹  (æ–¹æ¡ˆ2)  
3. **ç¬¬ä¸‰æ­¥**: å¼•å…¥å¯¹æ¯”å­¦ä¹  (æ–¹æ¡ˆ5)
4. **ç¬¬å››æ­¥**: å®Œæ•´æ··åˆæ¶æ„

### å…³é”®æŠ€æœ¯è¦ç‚¹

1. **æ³¨æ„åŠ›è®¾è®¡**: å¤šå¤´æ³¨æ„åŠ› + æ®‹å·®è¿æ¥
2. **æƒé‡åˆå§‹åŒ–**: Xavieråˆå§‹åŒ– + å°å­¦ä¹ ç‡
3. **æ¢¯åº¦å¤„ç†**: æ¢¯åº¦è£å‰ª + åˆ†åˆ«ä¼˜åŒ–ä¸åŒç»„ä»¶
4. **è¯„ä¼°æŒ‡æ ‡**: æ–°å¢ä»»åŠ¡ååŒåº¦ã€è¡¨ç¤ºç›¸ä¼¼æ€§ç­‰æŒ‡æ ‡
